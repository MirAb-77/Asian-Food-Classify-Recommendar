# -*- coding: utf-8 -*-
"""Food ML

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SPeb7EA8v48RMZXn1EMkXtcNpTv7qOIB
"""

import pandas as pd
import kagglehub

# Download dataset from Kaggle
dataset_path = kagglehub.dataset_download('sukhmandeepsinghbrar/indian-food-dataset')

print("Data source import complete.")

from google.colab import drive
drive.mount('/content/drive')

pip install googletrans==4.0.0-rc1



# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

"""# **Load Data**"""

import pandas as pd

# Define the correct path to the CSV file
csv_path = "/content/recipes.csv"

# Load the dataset
try:
    food = pd.read_csv(csv_path)
    print("Data Read Successfully")

    # Display the first few rows
    print(food.head())
except FileNotFoundError:
    print(f"Error: The file at {csv_path} was not found.")
except pd.errors.EmptyDataError:
    print("Error: The file is empty.")
except pd.errors.ParserError:
    print("Error: There was a problem parsing the file.")

# Define the correct path to the CSV file
csv_path = f"{dataset_path}/IndianFoodDatasetCSV.csv"

# Load the dataset
df = pd.read_csv(csv_path)
print("Data Read Successfully")

# Display the first few rows
df.head()

df.info()

df.isnull().any()

df.isnull().sum()

# Filter and display rows with missing values
missing_rows = df[df.isnull().any(axis=1)]
missing_rows

#Drop 'Ingredients' and 'RecipeName' columns
df = df.drop(columns=['Ingredients', 'RecipeName'])

# Rename columns
df = df.rename(columns={
    'TranslatedRecipeName': 'RecipeName',
    'TranslatedIngredients': 'Ingredients'
})

# Get the 'RecipeName' column for rows with missing values
missing_recipe_names = df.loc[df.isnull().any(axis=1), 'RecipeName']
print(missing_recipe_names)

# Dictionary of missing ingredients for specific recipes
missing_ingredients = {
    "Pear And Walnut Salad Recipe": "Pear, walnuts, lettuce, honey, lemon juice, olive oil, black pepper, salt, feta cheese",
    "Spinach and Cottage Cheese Eggless Ravioli Recipe": "Whole wheat flour, spinach, cottage cheese (paneer), garlic, onions, black pepper, nutmeg, olive oil, tomato sauce, dried basil, salt",
    "Thai Jasmine Sticky Rice Recipe": "Thai jasmine rice, coconut milk, sugar, salt, pandan leaves, mango",
    "Mashed Peas Recipe": "Green peas, butter, garlic, green chili, cumin, salt, lemon juice, fresh coriander leaves",
    "Classic Pavakkai Stir Fry Recipe (Bitter Gourd Fry)":
    "Bitter gourd (karela), mustard seeds, turmeric powder, red chili powder, coriander powder, cumin powder, fennel seeds, tamarind extract, jaggery, curry leaves, oil, salt, garlic, onions, green chilies, black pepper",

    "Urulaikizhangu Puli Thokku Recipe (South Indian Style Potatoes with Tamarind)":
    "Potatoes, mustard seeds, dry red chilies, tamarind extract, turmeric powder, red chili powder, coriander powder, cumin powder, garlic, curry leaves, sesame oil, salt, onions, asafoetida, fenugreek seeds"
}

# Fill in missing values only in 'Ingredients' column
for recipe, ingredients in missing_ingredients.items():
    df.loc[df["RecipeName"] == recipe, "Ingredients"] = ingredients


# Display the updated rows to verify
df.loc[df["RecipeName"].isin(missing_ingredients.keys()), ["RecipeName", "Ingredients"]]

# Get the 'RecipeName' column for rows with missing values
missing_recipe_names = df.loc[df.isnull().any(axis=1), 'RecipeName']
print(missing_recipe_names)

# Check for duplicate rows
duplicate_rows = df[df.duplicated()]
print(f"Total duplicate rows: {duplicate_rows.shape[0]}")

# If needed, display the first few duplicate rows
if not duplicate_rows.empty:
    print(duplicate_rows.head())

# Remove duplicate rows
df_cleaned = df.drop_duplicates()

# Confirm removal
print(f"Total rows after removing duplicates: {df_cleaned.shape[0]}")



"""# **Analysis**"""

df['Cuisine'].unique()

df['Diet'].unique()

df['Course'].unique()

"""# **Preprocessing**
## **What will we do next after looking at the data ?**

**We will try to remove the rows which are not completely translated to english to make it ready for ML suggestion model**
"""

print(df['Ingredients'].head(10).to_string(index=False))  # Shows only first 10 rows

# prompt: import re
# non_english_pattern = re.compile(r'[^\x00-\x7F]+')
# mask = df['Ingredients'].str.contains(non_english_pattern, na=False)
# food = df[~mask]  her ein code of notebook . sueonly rows of eacj h column in whch all are neglish . All textte in rows of each column shoudl eb english only onlys elcet those rows of eaxh column

import re

non_english_pattern = re.compile(r'[^\x00-\x7F]+')
mask = df['Ingredients'].str.contains(non_english_pattern, na=False)
food = df[~mask]

food.info()

food.to_csv("recipes.csv", index=False, encoding="utf-8")

food['Ingredients']

"""**Now we had successfully removed the rows which were not in english**

**Next step is to separate the Ingredients by commas in separate column to respective recipe**

## NLP for extracting ingredients

Natural Language Processing (NLP) techniques to extract the ingredients from the "TranslatedIngredients" column and create a new column with the ingredients separated by commas
"""

import pandas as pd
import spacy
import re

# Load spaCy English model
nlp = spacy.load("en_core_web_sm")

# Stopwords and unnecessary words in ingredient names
STOPWORDS = {"and", "with", "of", "to", "in", "for", "a", "an", "the", "chopped", "cup", "tablespoon", "teaspoon", "grams"}

# Function to clean and extract key ingredients
def extract_ingredients(text):
    if pd.isnull(text) or not isinstance(text, str):
        return ""  # Return empty string for missing values

    text = text.lower().strip()  # Normalize text (lowercase & remove extra spaces)
    text = re.sub(r'[\d]+', '', text)  # Remove numbers
    text = re.sub(r'[^\w\s,]', '', text)  # Remove punctuation except commas

    doc = nlp(text)

    # Extract meaningful ingredient words (NOUN, PROPN, compounds)
    ingredients = {
        token.lemma_ for token in doc
        if (token.pos_ in ["NOUN", "PROPN"] or token.dep_ in ["compound", "amod"])
        and token.text not in STOPWORDS
        and len(token.text) > 2  # Avoid single-character tokens
    }

    return ', '.join(sorted(ingredients))  # Return unique & sorted ingredients

# Apply function to extract ingredients
food['ExtractedIngredients'] = food['Ingredients'].apply(extract_ingredients)

# Save updated dataset
food.to_csv("final_cleaned_recipes.csv", index=False, encoding="utf-8")

print(food[['Ingredients', 'ExtractedIngredients']])

"""**We have extracted ingredients but some numbers, brackets, etc is still in the information, so we will try to remove that too**

"""

import re

def clean_ingredients(text):
    if isinstance(text, str):  # Ensure text is a string before processing
        text = re.sub(r'\d+', '', text)  # Remove numbers
        text = re.sub(r'[()\[\]/]', '', text)  # Remove unwanted brackets and slashes
        text = re.sub(r'[^a-zA-Z\s,]', '', text)  # Remove any special characters except spaces and commas
        text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces and trim
    return text

food['ExtractedIngredients'] = food['ExtractedIngredients'].apply(clean_ingredients)

food['ExtractedIngredients'].head()

food.info()

"""# **Recommendation System**
* Text Preprocessing: Perform text preprocessing steps such as lowercasing, removing punctuation, and tokenizing the ingredients.
* Term Frequency-Inverse Document Frequency (TF-IDF): Use TF-IDF vectorization to represent ingredients in numerical form, giving more weight to ingredients that are rare across recipes but common within a recipe.
* Cosine Similarity with Weighted Features: Calculate cosine similarity between the user-provided ingredients and recipe ingredients, but also consider other features such as preparation time, cooking time, and total time. Weight these features accordingly to balance their importance in the similarity calculation.
* Ranking with Combined Scores: Combine the similarity scores from ingredient vectors and other features, then rank the recipes based on the combined scores.
"""

import string
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

"""#### Text Preprocessing"""

import nltk

# Download required NLTK resources
nltk.download('punkt')  # For tokenization
nltk.download('stopwords')  # For stopwords
nltk.download('wordnet')  # For lemmatization

print("NLTK resources downloaded successfully!")

import nltk
nltk.data.path.append('/usr/share/nltk_data/')  # Ensure correct path

# Remove old downloads and reinstall
nltk.download('all', force=True)

print("All NLTK resources downloaded successfully!")

# Initialize Lemmatizer
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

# Define domain-specific stopwords for Indian food
indian_food_stopwords = {
    "curry", "masala", "gravy", "powder", "paste", "fried", "boiled", "dish",
    "chopped", "fresh", "dry", "ground", "whole", "spicy", "sweet", "hot"
}

# Synonym Mapping for Indian food ingredients
synonyms = {
    "coriander": "dhania",
    "yogurt": "curd",
    "brinjal": "eggplant",
    "green chilli": "hari mirch",
    "capsicum": "bell pepper",
    "cottage cheese": "paneer",
    "clarified butter": "ghee",
    "lentils": "dal",
    "fenugreek": "methi",
    "caraway": "shahi jeera",
    "mustard seeds": "rai"
}

# Function to clean and preprocess text
def preprocess_text(text):
    if not isinstance(text, str) or not text.strip():  # Handle empty values
        return ""

    # Convert to lowercase
    text = text.lower()

    # Remove punctuation
    text = re.sub(rf"[{re.escape(string.punctuation)}]", "", text)

    # Tokenize
    tokens = word_tokenize(text)

    # Load stopwords explicitly
    stop_words = set(stopwords.words('english')) | indian_food_stopwords

    # Remove stopwords and apply lemmatization
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]

    # Remove duplicates: Ensure only singular or plural form remains
    unique_tokens = list(set(tokens))

    return ' '.join(unique_tokens)

# Function to replace synonyms
def replace_synonyms(text):
    for key, value in synonyms.items():
        text = text.replace(key, value)
    return text

food['CleanedIngredients'].head()

"""#### TF-IDF Vectorization"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Define a more robust TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(
    stop_words='english',     # Remove common stopwords
    ngram_range=(1, 4),       # Use unigrams & bigrams
    max_df=0.95,              # Ignore words in >95% of docs (too common)
    min_df=0.01,                 # Ignore words in <2 docs (too rare)
    max_features=5000         # Limit feature set to top 5000 terms
)

# Fit & transform the CleanedIngredients column
tfidf_matrix = tfidf_vectorizer.fit_transform(food['CleanedIngredients'])

# Convert to DataFrame for better visualization
tfidf_df = pd.DataFrame(
    tfidf_matrix.toarray(),
    columns=tfidf_vectorizer.get_feature_names_out()
)

# Display the first few rows
tfidf_df.head()

"""#### Calculate Similarity with Weighted Features"""

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

def calculate_similarity(user_ingredients, user_prep_time, user_cook_time, weight_ing=0.5, weight_time=0.5):
    """
    Calculate similarity based on user ingredients, preparation time, and cooking time.

    Parameters:
    - user_ingredients (list): List of user-provided ingredients.
    - user_prep_time (int): User's available preparation time in minutes.
    - user_cook_time (int): User's available cooking time in minutes.
    - weight_ing (float): Weight for ingredient similarity (default 0.5).
    - weight_time (float): Weight for time-based similarity (default 0.5).

    Returns:
    - numpy array: Combined similarity scores for each recipe.
    """
    # Ensure weights sum to 1
    total_weight = weight_ing + weight_time
    weight_ing /= total_weight
    weight_time /= total_weight

    # Convert user ingredients into a TF-IDF vector
    user_ingredients_text = preprocess_text(', '.join(user_ingredients))
    user_tfidf = tfidf_vectorizer.transform([user_ingredients_text])

    # Compute ingredient similarity
    cosine_similarities = cosine_similarity(user_tfidf, tfidf_matrix)[0]

    # Normalize time-based features to avoid division errors
    max_prep_time = max(food['PrepTimeInMins'].max(), 1)  # Changed df to food
    max_cook_time = max(food['CookTimeInMins'].max(), 1)  # Changed df to food

    prep_time_similarity = 1 - abs(food['PrepTimeInMins'] - user_prep_time) / max_prep_time
    cook_time_similarity = 1 - abs(food['CookTimeInMins'] - user_cook_time) / max_cook_time

    # Handle missing values by replacing NaNs with 0
    prep_time_similarity = np.nan_to_num(prep_time_similarity)
    cook_time_similarity = np.nan_to_num(cook_time_similarity)

    # Compute combined similarity score
    combined_similarity = (weight_ing * cosine_similarities) + (weight_time * (prep_time_similarity + cook_time_similarity) / 2)

    return combined_similarity

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

def calculate_similarity(user_ingredients, user_prep_time, user_cook_time, weight_ing=0.5, weight_time=0.5):
    """
    Calculate similarity based on user ingredients, preparation time, and cooking time.

    Parameters:
    - user_ingredients (list): List of user-provided ingredients.
    - user_prep_time (int): User's available preparation time in minutes.
    - user_cook_time (int): User's available cooking time in minutes.
    - weight_ing (float): Weight for ingredient similarity (default 0.5).
    - weight_time (float): Weight for time-based similarity (default 0.5).

    Returns:
    - numpy array: Combined similarity scores for each recipe.
    """
    # Ensure weights sum to 1
    total_weight = weight_ing + weight_time
    weight_ing /= total_weight
    weight_time /= total_weight

    # Convert user ingredients into a TF-IDF vector without preprocessing
    user_ingredients_text = ', '.join(user_ingredients)
    user_tfidf = tfidf_vectorizer.transform([user_ingredients_text])

    # Compute ingredient similarity
    cosine_similarities = cosine_similarity(user_tfidf, tfidf_matrix)[0]

    # Normalize time-based features to avoid division errors
    max_prep_time = max(food['PrepTimeInMins'].max(), 1)
    max_cook_time = max(food['CookTimeInMins'].max(), 1)

    prep_time_similarity = 1 - abs(food['PrepTimeInMins'] - user_prep_time) / max_prep_time
    cook_time_similarity = 1 - abs(food['CookTimeInMins'] - user_cook_time) / max_cook_time

    # Handle missing values by replacing NaNs with 0
    prep_time_similarity = np.nan_to_num(prep_time_similarity)
    cook_time_similarity = np.nan_to_num(cook_time_similarity)

    # Compute combined similarity score
    combined_similarity = (weight_ing * cosine_similarities) + (weight_time * (prep_time_similarity + cook_time_similarity) / 2)

    return combined_similarity

"""#### Ranking with Combined Scores"""

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import pandas as pd

def recommend_recipes(user_ingredients, user_prep_time, user_cook_time, top_n=5):
    """
    Recommend top N recipes based on ingredient similarity and cooking time.
    Also, display a beautiful visualization of recommendations.

    Parameters:
    - user_ingredients (list): List of ingredients provided by the user.
    - user_prep_time (int): Available preparation time in minutes.
    - user_cook_time (int): Available cooking time in minutes.
    - top_n (int): Number of top recommendations to return (default: 5).

    Returns:
    - DataFrame: Top N recommended recipes with similarity scores.
    - Visualization: Lollipop plot & interactive bar chart.
    """
    if not user_ingredients:
        print("⚠️ Please provide at least one ingredient.")
        return None

    # Calculate combined similarity scores
    combined_similarity = calculate_similarity(user_ingredients, user_prep_time, user_cook_time)

    # Sort indices in descending order of similarity
    sorted_indices = combined_similarity.argsort()[::-1]

    # Get top N recommended recipes
    top_recommendations = food.iloc[sorted_indices[:top_n]].copy()
    top_recommendations['Similarity'] = combined_similarity[sorted_indices[:top_n]]

    # Select relevant columns
    top_recommendations = top_recommendations[['RecipeName', 'PrepTimeInMins', 'CookTimeInMins', 'Similarity']]

    # ✅ 🎨 **Lollipop Chart with Black Background**
    fig, ax = plt.subplots(figsize=(9, 5), facecolor="black")  # Increased size & black background

    ax.set_facecolor("black")  # Black background
    plt.grid(axis="x", linestyle="--", linewidth=0.5, color="gray", alpha=0.6)  # Subtle grid

    # Line (Stem) for Lollipop Chart
    for i, value in enumerate(top_recommendations["Similarity"]):
        plt.plot([0, value], [i, i], linestyle="--", color="gray", alpha=0.7)  # Dashed lines

    # Dots for Lollipop Chart
    sns.scatterplot(
        x=top_recommendations["Similarity"],
        y=top_recommendations["RecipeName"],
        s=100,
        color="cyan",
        edgecolor="magenta",
        linewidth=1.5
    )

    # Labels & Title
    plt.xlabel("🔍 Similarity Score", fontsize=11, fontweight='bold', color="white")
    plt.ylabel("🍽️ Recipe Name", fontsize=11, fontweight='bold', color="white")
    plt.title("🌟 Top Recommended Recipes (Lollipop Chart)", fontsize=13, fontweight='bold', color="white")

    plt.xticks(color="white")
    plt.yticks(color="white")
    plt.gca().invert_yaxis()  # Show highest similarity at the top
    plt.tight_layout()
    plt.show()

    # ✅ **Adding Space Before Plotly Graph**
    print("\n" * 3)  # Adds visual spacing before the next plot

    # ✅ 📊 **Interactive Plotly Bar Chart**
    fig = px.bar(
        top_recommendations,
        x="Similarity",
        y="RecipeName",
        orientation="h",
        text="Similarity",
        color="Similarity",
        color_continuous_scale="magma"  # Darker theme to match black bg
    )

    fig.update_layout(
        title="Top Recommended Recipes",
        xaxis_title="Similarity Score",
        yaxis_title="Recipe Name",
        plot_bgcolor="rgba(0,0,0,0)",
        font=dict(size=12),
        height=420  # Adjusted for consistency
    )

    fig.show()  # Display interactive Plotly chart

    return top_recommendations

"""## **Example Use**"""

user_ingredients = ["tomato", "onion", "garlic"]
user_prep_time = 20
user_cook_time = 30

recommendations = recommend_recipes(user_ingredients, user_prep_time, user_cook_time, top_n=5)
recommendations

import re
import string


user_ingredients = ["tomato", "onion", "garlic"]
user_prep_time = 20
user_cook_time = 30

recommendations = recommend_recipes(user_ingredients, user_prep_time, user_cook_time, top_n=5)
recommendations

"""### Print whole recipe

"""

for index in recommendations.index:
    recipe_info = food.loc[index]
    print("Recipe Information for Index", index)
    print(recipe_info)
    print("---------------------------------------------------------------------------------------------")

#Drop 'Ingredients' and 'RecipeName' columns
df = df.drop(columns=['TranslatedInstructions', 'CleanedIngredients '])

for index in recommendations.index:
    recipe_info = food.loc[index]
    print("Recipe Information for Index", index)
    print(recipe_info)
    print("---------------------------------------------------------------------------------------------")

food.info()

from googletrans import Translator
import pandas as pd
import time

translator = Translator()

# Function to translate text with retries
def translate_text(text, retries=3):
    if not isinstance(text, str) or text.strip() == "":  # Skip NaN or empty values
        return text

    for _ in range(retries):
        try:
            translation = translator.translate(text, src='hi', dest='en')
            if translation and translation.text:
                return translation.text
        except Exception as e:
            print(f"Retrying... Translation error: {e}")
            time.sleep(2)  # Add delay before retrying

    return text  # Return original text if translation fails

# Apply translation to the 'Instructions' column
food["Instructions"] = food["Instructions"].apply(translate_text)

# Display updated DataFrame
print(food)

"""## Complete Interface"""

from IPython.core.display import display, HTML

def generate_html_table(recommendations, df):
    """
    Generates a beautifully formatted HTML table for displaying recommended recipes.

    Parameters:
    - recommendations (DataFrame): The recommended recipes dataframe.
    - df (DataFrame): The original dataset containing full recipe details.

    Returns:
    - HTML object: Displays the formatted table.
    """

    if recommendations is None or recommendations.empty:
        return HTML("<p style='color: red; font-weight: bold;'>⚠️ No recipes found. Try different ingredients!</p>")

    # Define table styles for better visuals
    table_style = """
    <style>
        table {
            width: 100%;
            border-collapse: collapse;
            font-family: Arial, sans-serif;
            font-size: 14px;
            background-color: black; /* 🔥 Black background */
            color: white; /* 🔥 White text */
        }
        th {
            background-color: black; /* 🔥 Black background */
            color: white; /* 🔥 White text */
            padding: 10px;
            text-align: left;
            border-bottom: 2px solid #E0CDA9; /* Soft skin color */
        }
        td {
            padding: 8px;
            border: 1px solid #555; /* Dark border */
            background-color: #E0CDA9; /* 🌟 Skin-colored background */
            color: black; /* 🔥 Black text */
        }
        tr:nth-child(even) td {
            background-color: #D2B48C; /* Alternate skin tone */
        }
        tr:hover td {
            background-color: #C5A478; /* Darker shade on hover */
        }
        a {
            color: #FFD700; /* 🔥 Gold links */
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
    </style>
    """

    # Create table header
    html_table = f"{table_style}<table>"
    html_table += "<tr><th>#</th><th>Recipe Name</th><th>Ingredients</th><th>Prep Time (mins)</th><th>Cook Time (mins)</th><th>Total Time</th><th>Servings</th><th>Cuisine</th><th>Course</th><th>Diet</th><th>Instructions</th><th>URL</th></tr>"

    # Populate table rows
    for i, index in enumerate(recommendations.index, start=1):
        recipe_info = df.loc[index]

        html_table += f"""
        <tr>
            <td><b>{i}</b></td>
            <td>{recipe_info['RecipeName']}</td>
            <td>{recipe_info['Ingredients']}</td>
            <td>{recipe_info['PrepTimeInMins']}</td>
            <td>{recipe_info['CookTimeInMins']}</td>
            <td>{recipe_info['TotalTimeInMins']}</td>
            <td>{recipe_info['Servings']}</td>
            <td>{recipe_info['Cuisine']}</td>
            <td>{recipe_info['Course']}</td>
            <td>{recipe_info['Diet']}</td>
            <td>{recipe_info['Instructions'][:100]}...</td>  <!-- Show first 100 chars for readability -->
            <td><a href="{recipe_info['URL']}" target="_blank">🔗 View Recipe</a></td>
        </tr>
        """

    html_table += "</table>"

    return HTML(html_table)


# 🔥 Run the improved function
user_ingredients = ["onion", "tomato", "garlic", "ginger"]
user_prep_time = 30
user_cook_time = 45

display(generate_html_table(recommendations, df))

"""## Further Implementation :
**Features like Cuisine type, Diet and Course can be implemented.**
"""

def calculate_similarity(user_ingredients, user_prep_time, user_cook_time, cuisine=None, diet=None, course=None):
    # Process user ingredients
    user_ingredients_text = preprocess_text(', '.join(user_ingredients))
    user_tfidf = tfidf_vectorizer.transform([user_ingredients_text])
    cosine_similarities = cosine_similarity(user_tfidf, tfidf_matrix)[0]

    # Time-based similarity
    prep_time_similarity = 1 - abs(df['PrepTimeInMins'] - user_prep_time) / df['PrepTimeInMins'].max()
    cook_time_similarity = 1 - abs(df['CookTimeInMins'] - user_cook_time) / df['CookTimeInMins'].max()

    # Categorical Matching (Cuisine, Diet, Course)
    cuisine_similarity = (df['Cuisine'] == cuisine).astype(int) if cuisine else 1
    diet_similarity = (df['Diet'] == diet).astype(int) if diet else 1
    course_similarity = (df['Course'] == course).astype(int) if course else 1

    # Normalize similarities and align dimensions
    min_length = min(len(cosine_similarities), len(prep_time_similarity))
    cosine_similarities = cosine_similarities[:min_length]
    prep_time_similarity = prep_time_similarity[:min_length]
    cook_time_similarity = cook_time_similarity[:min_length]
    cuisine_similarity = cuisine_similarity[:min_length]
    diet_similarity = diet_similarity[:min_length]
    course_similarity = course_similarity[:min_length]

    # Weighted Combination
    combined_similarity = (
        (cosine_similarities * 0.3) +  # Ingredient similarity (40%)
        (prep_time_similarity * 0.15) +  # Prep time (15%)
        (cook_time_similarity * 0.15) +  # Cook time (15%)
        (cuisine_similarity * 0.1) +  # Cuisine match (10%)
        (diet_similarity * 0.1) +  # Diet match (10%)
        (course_similarity * 0.1)  # Course match (10%)
    )

    return combined_similarity


def recommend_recipes(user_ingredients, user_prep_time, user_cook_time, cuisine=None, diet=None, course=None, top_n=5):
    combined_similarity = calculate_similarity(user_ingredients, user_prep_time, user_cook_time, cuisine, diet, course)

    # Sort recipes by similarity
    sorted_indices = combined_similarity.argsort()[::-1]

    # Select top_n recipes
    top_recommendations = df.iloc[sorted_indices[:top_n], df.columns.get_indexer([
        'RecipeName', 'Ingredients', 'PrepTimeInMins',
        'CookTimeInMins', 'TotalTimeInMins', 'Servings', 'Cuisine', 'Course', 'Diet', 'Instructions', 'URL'
    ])].copy()

    return top_recommendations


# Test the improved function
user_ingredients = ["onion", "tomato", "garlic", "ginger"]
user_prep_time = 30
user_cook_time = 45
preferred_cuisine = "Indian"
preferred_diet = "Vegetarian"
preferred_course = "Main Course"

recommendations = recommend_recipes(user_ingredients, user_prep_time, user_cook_time, preferred_cuisine, preferred_diet, preferred_course)

# Display recommendations in a beautiful HTML table
display(generate_html_table(recommendations, df))

food['Cuisine'].unique()